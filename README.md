# Understanding the Figurative Language Capabilities of Large Language Models

Large language models have made significant strides in natural language processing, but they still face difficulties in understanding figurative language. This essay explores the importance of scoreboarding and post-training techniques to improve the figurative language capabilities of large language models. We discuss the use of probability distributions and pseudo-labeling to develop common ground between language models over the scoreboard of a conversation, and propose a solution that involves training the model on a diverse dataset of figurative language, using post-training techniques to fine-tune the model, and evaluating its performance using scoreboarding. We also suggest several datasets that can be used to train and evaluate the modelâ€™s performance in recognizing and generating figurative language in different contexts. Improving the figurative language capabilities of large language models has the potential to improve their communication abilities and lead to more advanced natural language processing systems.
